import streamlit as st
from st_screen_stats import ScreenData
import asyncio
import json
from openai import AsyncOpenAI, OpenAI
import os
import aiohttp
import requests
import re
from dotenv import load_dotenv
from google.auth import default
import google.auth.transport.requests

load_dotenv()

if not os.getenv("GOOGLE_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
if not os.getenv("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

from transformers import AutoTokenizer
from PIL import Image
import base64

# --- í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(
    layout="wide", 
    page_title="ëª¨ë¸ ì§€ë¬¸ ìƒì„± ë·°ì–´",
    initial_sidebar_state="collapsed"
)

# --- ë¡œê³ ë¥¼ base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ HTMLì— ì§ì ‘ ì‚½ì… ---
def get_base64_of_bin_file(bin_file):
    with open(bin_file, 'rb') as f:
        data = f.read()
    return base64.b64encode(data).decode()

# ë¡œê³  ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
img_base64 = get_base64_of_bin_file("logo_kangnam_202111.png")


# --- ì‚¬ìš©ì ì •ì˜ CSS ---
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Nanum+Myeongjo:wght@400;700;800&display=swap');

h3 {
    font-family: 'Nanum Myeongjo', serif !important;
    font-weight: 700;
}
.passage-container {
    height: 80vh;
    overflow-y: auto;
    border: 1px solid #e0e0e0;
    border-radius: 5px;
    padding: 15px;
}
.passage-font {
    border: 0.5px solid black;
    border-radius: 0px;
    padding: 10px;
    margin-bottom: 20px;
    font-family: 'Nanum Myeongjo', serif !important;
    line-height: 1.7;
    letter-spacing: -0.01em;
    font-weight: 500;
}
.passage-font p {
    text-indent: 1em; /* ê° ë¬¸ë‹¨ì˜ ì²« ì¤„ ë“¤ì—¬ì“°ê¸° */
    margin-bottom: 0em;
}
.passage-font-no-border {
    padding: 10px;
    margin-bottom: 20px;
    font-family: 'Nanum Myeongjo', serif !important;
    line-height: 1.7;
    letter-spacing: -0.01em;
    font-weight: 500;
}
.passage-font-no-border p {
    text-indent: 1em; /* ê° ë¬¸ë‹¨ì˜ ì²« ì¤„ ë“¤ì—¬ì“°ê¸° */
    margin-bottom: 0em;
}
.question-font {
    font-family: 'Nanum Myeongjo', serif !important;
    line-height: 1.7em;
    letter-spacing: -0.01em;
    font-weight: 500;
    margin-bottom: 1.5em;
}
.final-response-container {
    height: 600px;  /* ê¸°ë³¸ê°’, ë™ì ìœ¼ë¡œ ë®ì–´ì”Œì›Œì§ */
    overflow-y: auto;
    border: 0.5px solid black;
    border-radius: 0px;
    padding: 5px;
    background-color: white;
}
</style>
""", unsafe_allow_html=True)


# --- ì„¤ì •ê°’ ---
DATASET_PATH = "Gemini-sft-09-07-val.jsonl"
# Vertex AI ì¡°ì •ëœ ëª¨ë¸ ì„¤ì •
ENDPOINT_ID = "4075215603537805312"  # ì‚¬ìš©ì ì§€ì • ì—”ë“œí¬ì¸íŠ¸ ID (ksat-exp-09-06-flash)
PROJECT_ID = "gen-lang-client-0921402604"  # GCP í”„ë¡œì íŠ¸ ID  
LOCATION = "us-central1"  # ëª¨ë¸ì´ ë°°í¬ëœ ë¦¬ì „
MODEL_ID = "6275144856671092736"  # ì‹¤ì œ ëª¨ë¸ ID (ksat-exp-09-06-flash)

# ê¸°ì¡´ OpenAI ëª¨ë¸ë“¤ (ì°¸ê³ ìš©)
#MODEL_NAME = "ft:gpt-4.1-2025-04-14:ksat-agent:ksat-exp-09-06-large:CCMOwou1"
EXPERT_MODEL_NAME = "gemini-2.5-flash"

EXPERT_PROMPT = """
ë‹¹ì‹ ì€ ì‘ê°€ ëª¨ë¸ì—ê²Œ ìˆ˜ëŠ¥ ì§€ë¬¸ì„ ì‘ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ ëª¨ë¸ì…ë‹ˆë‹¤.
ì‘ê°€ ëª¨ë¸ì´ ìš”ì²­í•˜ëŠ” ì •ë³´ë¥¼ ì•„ë˜ì˜ ì§€ì¹¨ì— ë”°ë¼ ì œê³µí•´ ì£¼ì„¸ìš”.
1. íŠ¹ì • ê°œë…ì´ë‚˜ ì¸ë¬¼ì˜ ì£¼ì¥ì„ ì—¬ëŸ¬ ê°œ ì œì‹œí•  ë•Œì—ëŠ” ë²™ë ¬ì ìœ¼ë¡œ ë‚˜ì—´í•˜ì§€ ë§ê³  ê° ê°œë… ë˜ëŠ” ì£¼ì¥ì˜ ê³µí†µì  ë° ì°¨ì´ì ì´ ë°œìƒí•˜ëŠ” ëŒ€ë¦½ì ì„ ëª…í™•íˆ í•˜ì—¬ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.
2. ê³¼í•™ì /ê²½ì œì  ì›ë¦¬ ë˜ëŠ” ê¸°ìˆ ì˜ ì‘ë™ ì›ë¦¬ë¥¼ ì œì‹œí•  ë•Œì—ëŠ” ì›ë¦¬ë¥¼ í”¼ìƒì /ê´‘ë²”ìœ„í•˜ê²Œ ë‚˜ì—´í•˜ê¸° ë§ê³ , ë¯¸ì‹œì ì´ê³  ê¹Šì´ ìˆê²Œ ì„¤ëª…í•˜ì—¬ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.
3. ë²•ì  ê·œì •ì´ë‚˜ ì œë„ì˜ ì‘ë™ ì›ë¦¬ë¥¼ ì œì‹œí•  ë•Œì—ëŠ” ê·œì •ì´ë‚˜ ì œë„ë¥¼ ì¤„ì¤„ì´ ë‚˜ì—´í•˜ê¸°ë³´ë‹¤ëŠ”, í•´ë‹¹ ê·œì •ì´ ë“±ì¥í•œ ë°°ê²½ê³¼ ëª©ì (ë˜ëŠ” í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œ), ê·œì • ë˜ëŠ” ì œë„ê°€ í•´ë‹¹ ëª©ì  ë‹¬ì„±ì„ ìœ„í•´ ì‘ë™í•˜ëŠ” ì›ë¦¬ì— ì´ˆì ì„ ë§ì¶° ì„¤ëª…í•´ ì£¼ì„¸ìš”.
4. ì‘ê°€ ëª¨ë¸ì´ ë‹¤ì†Œ ê´‘ë²”ìœ„í•œ ì£¼ì œì— ëŒ€í•´ ì§ˆë¬¸í•œë‹¤ë©´, ì‘ê°€ ëª¨ë¸ì´ ì§€ë¬¸ì— í¬í•¨í•  ë‚´ìš©ì„ íƒìƒ‰í•˜ê³  ìˆëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒì˜ ì„œì‚¬ êµ¬ì¡° ì¤‘ í•˜ë‚˜ë¥¼ ì‚´ë ¤ ì…ì²´ì ìœ¼ë¡œ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.
    - ë¬¸ì œ ë°œìƒ ë° í•´ê²° êµ¬ì¡° : ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ì›ë¦¬ì™€ ê·¸ ì›ë¦¬ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ìˆ˜ë‹¨ê³¼ ë°©ë²•ì„ ì œì‹œí•˜ê³ , ê·¸ ì›ë¦¬ë¥¼ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤. ì¶”ê°€ë¡œ í•´ê²°ë²•ì˜ í•œê³„ì™€ ê·¸ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ë‹¤ë¥¸ ë°©ë²•ì„ ì œì‹œí•˜ëŠ” ê²ƒë„ ë°”ëŒì§í•©ë‹ˆë‹¤.
    - ë‹¤ì–‘í•œ ê²¬í•´ ë¹„êµ êµ¬ì¡° : í•˜ë‚˜ì˜ í™”ì œì— ëŒ€í•œ ì—¬ëŸ¬ ì¸ë¬¼ì´ë‚˜ í•™íŒŒì˜ ê´€ì ì„ ìˆœì°¨ì ìœ¼ë¡œ ì œì‹œí•˜ê³ , ê·¸ë“¤ì˜ í•´ì„ê³¼ ì£¼ì¥ì˜ ì°¨ì´ì , ë•Œë¡œëŠ” ì„œë¡œì˜ ê²¬í•´ì— ëŒ€í•œ ë¹„íŒê³¼ ê·¸ì— ëŒ€í•œ ë°˜ë°•ì„ ëª…í™•í•˜ê²Œ ë“œëŸ¬ëƒ…ë‹ˆë‹¤. ì´ êµ¬ì¡°ëŠ” ê° ê²¬í•´ì˜ í•µì‹¬ ë‚´ìš©ì„ ì •í™•íˆ íŒŒì•…í•˜ê³  ì„œë¡œ ë¹„êµ/ëŒ€ì¡°í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
    - ê°œë…(ë˜ëŠ” ì¡°ê±´) ë° ì ìš© ì‚¬ë¡€ êµ¬ì¡° : íŠ¹ì • ê°œë…ì´ë‚˜ ì œë„ë¥¼ ì •ì˜í•œ ë’¤, ì´ì™€ ê´€ë ¨ëœ ë²•ë¥ ì´ë‚˜ ê·œì¹™ì˜ êµ¬ì²´ì ì¸ ì¡°í•­ê³¼ ì¡°ê±´ì„ ìƒì„¸íˆ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ í•´ë‹¹ êµ¬ì¡°ê°€ ì ìš©ë  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ì œì‹œí•´ë„ ì¢‹ìŠµë‹ˆë‹¤. ì´ êµ¬ì¡°ëŠ” ì§€ë¬¸ì˜ ì •ë³´ë¥¼ êµ¬ì²´ì ì¸ ì‚¬ë¡€ì— ì ìš©í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
5. ìˆ˜ëŠ¥ì€ ë°°ê²½ì§€ì‹ì´ ì•„ë‹Œ ë…¼ë¦¬ì  ê·œì¹™ì„ ì´í•´í•˜ê³  ì ìš©í•˜ëŠ” ì‹œí—˜ì…ë‹ˆë‹¤. ê°œë…ì˜ ì–‘ê³¼ ë‹¤ì–‘ì„±ë³´ë‹¤ëŠ” ì¡°ê±´, ì¸ê³¼, ëŒ€ë¦½, ë¶„ê¸°, ìœ„ê³„ ë“± ë…¼ë¦¬ì  ê·œì¹™ì„ ëª…í™•íˆ í•˜ì—¬ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.
6. ìˆ˜ì‹ì„ í†µí•œ ì„¤ëª…ë³´ë‹¤ëŠ”, ì–¸ì–´ë¥¼ í™œìš©í•˜ì—¬ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
7. ë°°ê²½ ì§€ì‹ ìˆ˜ì¤€ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³ ë ¤í•˜ì—¬ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.
    - ìˆ˜í•™: ì‚¬ì¹™ì—°ì‚°ê³¼ ê±°ë“­ì œê³± ì •ë„ì˜ ê¸°ì´ˆì ì¸ ìˆ˜í•™ ì§€ì‹ë§Œì„ ê°–ì¶˜ ë…ìë¥¼ ì „ì œë¡œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.
    - ê³¼í•™: í˜, ì†ë„, ê±°ë¦¬, ë¶„ì, ì›ì, ë°”ì´ëŸ¬ìŠ¤, ë¯¸ìƒë¬¼ ë“± ê¸°ì´ˆì ì¸ ê°œë…ë§Œì„ ê°–ì¶˜ ë…ìë¥¼ ì „ì œí•´ì•¼ í•©ë‹ˆë‹¤.
    - ì‚¬íšŒ: í™”í, ê²½ê¸°, í†µí™”, ì±„ê¶Œ, ë¯¼ë²•, êµ­íšŒ, ë³´ë„ ë“± ê¸°ì´ˆì ì¸ ì‚¬íšŒ ìš©ì–´ë¥¼ ìˆ™ì§€í•œ ë…ìë¥¼ ì „ì œí•©ë‹ˆë‹¤. 
8. í•´ë‹¹ ë„ë©”ì¸ì˜ ì „ë¬¸/íŠ¹ìˆ˜ ìš©ì–´ ëŒ€ì‹ , ì¼ìƒì ì¸ ìš©ì–´ì™€ ì‚¬ë¡€ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.
9. ì •ë³´ëŠ” ìš”ì²­ì— ì¶©ì‹¤í•˜ë˜ ê°„ê²°í•˜ê²Œ í•µì‹¬ë§Œ ì œê³µí•´ ì£¼ì„¸ìš”.
"""

# --- Vertex AI ì¡°ì •ëœ ëª¨ë¸ í˜¸ì¶œ í•¨ìˆ˜ ---
def get_vertex_ai_credentials():
    """Vertex AI ì¸ì¦ í† í°ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        # 1. Streamlit Cloud Secretsì—ì„œ JSON ë¬¸ìì—´ë¡œ ì œê³µë˜ëŠ” ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ì²˜ë¦¬
        if hasattr(st, 'secrets') and "GOOGLE_APPLICATION_CREDENTIALS_JSON" in st.secrets:
            import json
            from google.oauth2 import service_account
            
            try:
                creds_json = json.loads(st.secrets["GOOGLE_APPLICATION_CREDENTIALS_JSON"])
                credentials = service_account.Credentials.from_service_account_info(
                    creds_json, 
                    scopes=["https://www.googleapis.com/auth/cloud-platform"]
                )
                credentials.refresh(google.auth.transport.requests.Request())
                return credentials.token
            except Exception as e:
                st.error(f"Streamlit Secrets ì¸ì¦ ì‹¤íŒ¨: {e}")
        
        # 2. ë¡œì»¬ í™˜ê²½ì—ì„œ gcloud CLI ì¸ì¦ ì‹œë„
        original_creds = os.environ.pop('GOOGLE_APPLICATION_CREDENTIALS', None)
        
        try:
            credentials, _ = default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
            credentials.refresh(google.auth.transport.requests.Request())
            return credentials.token
        finally:
            if original_creds:
                os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = original_creds
                
    except Exception as e:
        # 3. gcloud ì¸ì¦ ì‹¤íŒ¨ ì‹œ, í™˜ê²½ë³€ìˆ˜ ì¸ì¦ ì¬ì‹œë„
        if 'original_creds' in locals() and original_creds and os.path.exists(original_creds):
            try:
                os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = original_creds
                credentials, _ = default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
                credentials.refresh(google.auth.transport.requests.Request())
                return credentials.token
            except Exception as e2:
                pass
        
        # ì¸ì¦ ë°©ë²• ì•ˆë‚´
        st.error(f"Vertex AI ì¸ì¦ ì‹¤íŒ¨: {e}")
        with st.expander("ğŸ”§ ì¸ì¦ ì„¤ì • ë°©ë²•", expanded=True):
            st.markdown("""
            **Streamlit Cloud ë°°í¬ ì‹œ:**
            1. Settings > Secretsì—ì„œ ë‹¤ìŒ ì¶”ê°€:
            ```toml
            GOOGLE_APPLICATION_CREDENTIALS_JSON = '''
            {
              "type": "service_account",
              "project_id": "your-project-id",
              ...ì „ì²´ ì„œë¹„ìŠ¤ ê³„ì • í‚¤ JSON...
            }
            '''
            ```
            
            **ë¡œì»¬ ê°œë°œ ì‹œ:**
            ```bash
            gcloud auth application-default login
            ```
            
            **ë˜ëŠ” ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼:**
            ```bash
            export GOOGLE_APPLICATION_CREDENTIALS="í‚¤íŒŒì¼ê²½ë¡œ.json"
            ```
            """)
        return None

async def call_vertex_ai_endpoint(endpoint_id: str, project_id: str, location: str, messages: list, temperature: float = 0.7):
    """Vertex AI ì¡°ì •ëœ ëª¨ë¸ ì—”ë“œí¬ì¸íŠ¸ì— ì§ì ‘ ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤."""
    access_token = get_vertex_ai_credentials()
    if not access_token:
        return None
    
    # Vertex AI ì—”ë“œí¬ì¸íŠ¸ URL
    url = f"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{endpoint_id}:generateContent"
    
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Content-Type": "application/json"
    }
    
    # Gemini API í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ ë³€í™˜
    contents = []
    system_instruction = None
    
    for msg in messages:
        if msg["role"] == "system":
            system_instruction = msg["content"]
        elif msg["role"] == "user":
            contents.append({
                "role": "user",
                "parts": [{"text": msg["content"]}]
            })
        elif msg["role"] == "assistant":
            contents.append({
                "role": "model", 
                "parts": [{"text": msg["content"]}]
            })
    
    # ìš”ì²­ í˜ì´ë¡œë“œ
    payload = {
        "contents": contents,
        "generationConfig": {
            "temperature": temperature,
            "maxOutputTokens": 8192,
        }
    }
    
    if system_instruction:
        payload["systemInstruction"] = {
            "parts": [{"text": system_instruction}]
        }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(url, headers=headers, json=payload) as response:
                if response.status == 200:
                    result = await response.json()
                    if "candidates" in result and result["candidates"]:
                        content = result["candidates"][0]["content"]["parts"][0]["text"]
                        return content
                    else:
                        return "[error] No response from model"
                else:
                    error_text = await response.text()
                    return f"[error] HTTP {response.status}: {error_text}"
    except Exception as e:
        return f"[error] Request failed: {e}"

def call_vertex_ai_endpoint_sync(endpoint_id: str, project_id: str, location: str, messages: list, temperature: float = 0.7):
    """ë™ê¸°ì‹ Vertex AI ì¡°ì •ëœ ëª¨ë¸ í˜¸ì¶œ"""
    access_token = get_vertex_ai_credentials()
    if not access_token:
        return "[error] Authentication failed"
    
    url = f"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{endpoint_id}:generateContent"
    
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Content-Type": "application/json"
    }
    
    # Gemini API í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ ë³€í™˜
    contents = []
    system_instruction = None
    
    for msg in messages:
        if msg["role"] == "system":
            system_instruction = msg["content"]
        elif msg["role"] == "user":
            contents.append({
                "role": "user",
                "parts": [{"text": msg["content"]}]
            })
        elif msg["role"] == "assistant":
            contents.append({
                "role": "model", 
                "parts": [{"text": msg["content"]}]
            })
    
    payload = {
        "contents": contents,
        "generationConfig": {
            "temperature": temperature,
            "maxOutputTokens": 8192,
        }
    }
    
    if system_instruction:
        payload["systemInstruction"] = {
            "parts": [{"text": system_instruction}]
        }
    
    try:
        response = requests.post(url, headers=headers, json=payload)
        if response.status_code == 200:
            result = response.json()
            if "candidates" in result and result["candidates"]:
                content = result["candidates"][0]["content"]["parts"][0]["text"]
                return content
            else:
                return "[error] No response from model"
        else:
            return f"[error] HTTP {response.status_code}: {response.text}"
    except Exception as e:
        return f"[error] Request failed: {e}"

# --- ì¼ë°˜ Gemini API í´ë¼ì´ì–¸íŠ¸ (Expertìš©) ---
def create_openai_client(use_vertex_ai: bool = False, project_id: str = "", location: str = "") -> AsyncOpenAI:
    """Expertìš© ì¼ë°˜ Gemini API í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    api_key = os.getenv("GOOGLE_API_KEY")
    if api_key:
        return AsyncOpenAI(
            api_key=api_key,
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
        )
    else:
        st.error("GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.info("ğŸ’¡ Google AI Studioì—ì„œ API í‚¤ë¥¼ ìƒì„±í•˜ê³  í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”: https://aistudio.google.com/app/apikey")
        return AsyncOpenAI()

def create_sync_openai_client(use_vertex_ai: bool = False, project_id: str = "", location: str = "") -> OpenAI:
    """Expertìš© ë™ê¸°ì‹ ì¼ë°˜ Gemini API í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    api_key = os.getenv("GOOGLE_API_KEY")
    if api_key:
        return OpenAI(
            api_key=api_key,
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
        )
    else:
        return OpenAI()

# --- ë„ìš°ë¯¸ í•¨ìˆ˜ ---
@st.cache_data
def get_dataset_info(path):
    """ë°ì´í„°ì…‹ íŒŒì¼ì˜ ì´ ìƒ˜í”Œ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        with open(path, "r", encoding='utf-8') as f:
            return len(f.readlines())
    except FileNotFoundError:
        return 0

@st.cache_data
def load_sample(index):
    """ì§€ì •ëœ ì¸ë±ìŠ¤ì˜ ë°ì´í„°ì…‹ ìƒ˜í”Œì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(DATASET_PATH, "r", encoding='utf-8') as f:
            line = f.readlines()[index]
            data = json.loads(line)
            
            # Gemini í˜•ì‹ì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸, ê¸°ëŒ€ ì‘ë‹µ ì¶”ì¶œ
            system_prompt = ""
            user_prompt = ""
            expected_response = ""
            
            # systemInstructionì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
            if "systemInstruction" in data:
                system_instruction = data["systemInstruction"]
                if isinstance(system_instruction, dict) and "parts" in system_instruction:
                    if system_instruction["parts"] and "text" in system_instruction["parts"][0]:
                        system_prompt = system_instruction["parts"][0]["text"]
            
            # contentsì—ì„œ userì™€ model ë©”ì‹œì§€ ì¶”ì¶œ
            if "contents" in data:
                contents = data["contents"]
                for content in contents:
                    role = content.get("role", "")
                    parts = content.get("parts", [])
                    
                    if role == "user" and parts and "text" in parts[0]:
                        if not user_prompt:  # ì²« ë²ˆì§¸ user ë©”ì‹œì§€ë¥¼ ì‚¬ìš©
                            user_prompt = parts[0]["text"]
                    
                    elif role == "model" and parts and "text" in parts[0]:
                        expected_response_raw = parts[0]["text"]
                        # <passage> íƒœê·¸ ë‚´ìš© ì¶”ì¶œ ë˜ëŠ” ì „ì²´ ë‚´ìš© ì‚¬ìš©
                        if "<passage>" in expected_response_raw and "</passage>" in expected_response_raw:
                            start = expected_response_raw.find("<passage>") + len("<passage>")
                            end = expected_response_raw.find("</passage>")
                            expected_response = expected_response_raw[start:end].strip()
                        else:
                            # </think> ì´í›„ ë‚´ìš© ì¶”ì¶œ
                            if "</think>" in expected_response_raw:
                                expected_response = expected_response_raw.split("</think>", 1)[1].strip()
                            else:
                                expected_response = expected_response_raw.strip()
            
            return system_prompt, user_prompt, expected_response
    except Exception as e:
        st.error(f"ë°ì´í„°ì…‹ ë¡œë”© ì˜¤ë¥˜: {e}")
        return None, None, None

def format_text_to_html(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ ì¤„ë°”ê¿ˆì„ HTML ë‹¨ë½(<p>)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    paragraphs = text.strip().split('\n')
    html_paragraphs = [f"<p>{p.strip()}</p>" for p in paragraphs if p.strip()]
    return "".join(html_paragraphs)

def parse_prompt_structure(user_prompt: str) -> tuple[str, str, str]:
    """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ë¶„ì•¼, ìœ í˜•, ì£¼ì œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        lines = user_prompt.strip().split('\n')
        field_info = ""
        type_info = ""
        topic_info = ""
        
        for line in lines:
            line = line.strip()
            if line.startswith("ë¶„ì•¼:"):
                field_info = line.replace("ë¶„ì•¼:", "").strip()
            elif line.startswith("ìœ í˜•:"):
                type_info = line.replace("ìœ í˜•:", "").strip()
            elif line.startswith("ì£¼ì œ:"):
                topic_info = line.replace("ì£¼ì œ:", "").strip()
        
        return field_info, type_info, topic_info
    except Exception:
        return "íŒŒì‹± ì‹¤íŒ¨", "íŒŒì‹± ì‹¤íŒ¨", "íŒŒì‹± ì‹¤íŒ¨"

def format_prompt_from_components(field: str, type_info: str, topic: str) -> str:
    """ë¶„ì•¼, ìœ í˜•, ì£¼ì œë¥¼ ê²°í•©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return f"ë¶„ì•¼: {field}\nìœ í˜•: {type_info}\nì£¼ì œ: {topic}"


# --- ì‚¬ì´ë“œë°” (ë†’ì´ ìë™ ê°ì§€) ---
with st.sidebar:
    # ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì´ ì•„ë‹ ë•Œë§Œ ë†’ì´ ê°ì§€
    if not st.session_state.get("is_streaming", False):
        try:
            with st.container(border=False, height=1):
                screen_data = ScreenData()
                stats = screen_data.st_screen_data()  # ì»´í¬ë„ŒíŠ¸ ë¡œë”© ë° ê°’ ê°€ì ¸ì˜¤ê¸°

            if stats and "innerHeight" in stats:
                height = stats.get("innerHeight")
                if height is not None and isinstance(height, (int, float)) and height > 0:
                    # ì„¸ì…˜ ìƒíƒœì— ìµœì‹  ë†’ì´ ì €ì¥/ì—…ë°ì´íŠ¸ (í˜„ì¬ ë†’ì´ì™€ ë‹¤ë¥¼ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸)
                    if st.session_state.get("viewport_height") != height:
                        st.session_state.viewport_height = height
                else:
                    pass
        except Exception as e:
            pass
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì„¸ì…˜ ìƒíƒœì— viewport_heightê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
            if "viewport_height" not in st.session_state:
                st.session_state.viewport_height = 800  # ê¸°ë³¸ê°’ ì„¤ì •

    # ë†’ì´ ê°ì§€ëŠ” í•˜ì§€ë§Œ í‘œì‹œí•˜ì§€ ì•ŠìŒ

# --- ë©”ì¸ í˜ì´ì§€ ë¡œê³  & íƒ€ì´í‹€ (ìƒë‹¨) ---
st.markdown(f"""
<div style="display: flex; justify-content: flex-start; align-items: center; margin-bottom: 20px;">
    <img src="data:image/png;base64,{img_base64}" 
         style="width: 170px; height: auto; pointer-events: none; user-select: none; margin-right: 30px;" 
         alt="ê°•ë‚¨ëŒ€ì„±ìˆ˜ëŠ¥ì—°êµ¬ì†Œ ë¡œê³ ">
    <div class="passage-font-no-border" style="margin: 0; padding: 0; font-size: 40px; font-weight: 700;">
        AI Model Preview
    </div>
</div>
""", unsafe_allow_html=True)

# --- ë™ì  ë†’ì´ ê³„ì‚° ---
# ê¸°ë³¸ ë†’ì´ 600px, ê°ì§€ëœ ë†’ì´ê°€ ìˆìœ¼ë©´ ì ì ˆíˆ ì¡°ì ˆ
default_height = 600
if "viewport_height" in st.session_state:
    # í™”ë©´ ë†’ì´ì˜ 70%ë¥¼ ì»¨í…Œì´ë„ˆ ë†’ì´ë¡œ ì‚¬ìš© (í—¤ë”, ì—¬ë°± ë“±ì„ ê³ ë ¤)
    dynamic_height = int(st.session_state.viewport_height * 0.8)
    # ìµœì†Œ 400px, ìµœëŒ€ 1000pxë¡œ ì œí•œ
    container_height = max(400, min(dynamic_height, 1000))
else:
    container_height = default_height

# --- ë™ì  CSS ì ìš© ---
st.markdown(f"""
<style>
.final-response-container {{
    height: {container_height}px !important;
}}
</style>
""", unsafe_allow_html=True)


# --- 3ì»¬ëŸ¼ ë ˆì´ì•„ì›ƒ ---
col1, col2, col3 = st.columns([1, 1, 1])

# ì²« ë²ˆì§¸ ì»¬ëŸ¼: ì„¤ì • ë° ì…ë ¥
with col1:
    st.markdown("#### 1. ì„¤ì • ë° ì…ë ¥")
    with st.container(border=True, height=container_height):        
        # ëª¨ë¸ ì„ íƒ ì„¹ì…˜
        with st.container(border=True):
            st.markdown("**AI ëª¨ë¸ ì„ íƒ**")
            model_name = st.selectbox("ëª¨ë¸ëª…", ["KSAT Psg Flash (Preview 0908)"], index=0)
        
        # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì„¹ì…˜
        with st.container(border=True):
            st.markdown("**ì…ë ¥ í”„ë¡¬í”„íŠ¸**")
            tab1, tab2 = st.tabs(["Preset", "Custom"])
            
            with tab1:
                # ë°ì´í„°ì…‹ ìƒ˜í”Œ ì„ íƒ
                total_samples = get_dataset_info(DATASET_PATH)
                if total_samples > 0:
                    dataset_index = st.selectbox(
                        "ê²€ì¦ ë°ì´í„°ì…‹ ìƒ˜í”Œ",
                        options=range(total_samples),
                        format_func=lambda x: f"Sample #{x}",
                        index=0
                    )
                    
                    # ì„ íƒëœ ìƒ˜í”Œ ë¡œë“œ ë° íŒŒì‹±
                    system_prompt, user_prompt, expected_response = load_sample(dataset_index)
                    if user_prompt:
                        field_info, type_info, topic_info = parse_prompt_structure(user_prompt)
                        
                        # íŒŒì‹±ëœ ì •ë³´ í‘œì‹œ (ì½ê¸° ì „ìš©)
                        st.text_input("ë¶„ì•¼", value=field_info, disabled=True, key="preset_field_display")
                        st.text_input("ìœ í˜•", value=type_info, disabled=True, key="preset_type_display")
                        st.text_area("ì£¼ì œ", value=topic_info, disabled=True, height=100, key="preset_topic_display")
                        
                        # ì›ë³¸ ì§€ë¬¸ ìµìŠ¤íŒ¬ë”
                        with st.expander("ì›ë³¸ ì§€ë¬¸", expanded=False):
                            if expected_response:
                                st.markdown(f'<div class="passage-font">{format_text_to_html(expected_response)}</div>', unsafe_allow_html=True)
                        
                        # Preset íƒ­ ì‹¤í–‰ ë²„íŠ¼
                        preset_run_button = st.button("ì§€ë¬¸ ìƒì„±", type="primary", use_container_width=True, key="preset_run")
                    else:
                        st.error("ì„ íƒëœ ìƒ˜í”Œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        preset_run_button = False
                else:
                    st.error("ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    preset_run_button = False
            
            with tab2:
                # ì»¤ìŠ¤í…€ ì…ë ¥
                # ë¶„ì•¼ ì„ íƒ (ëŒ€ë¶„ì•¼-ì†Œë¶„ì•¼ êµ¬ì¡°)
                custom_major_field = st.selectbox("ëŒ€ë¶„ì•¼", ["ì¸ë¬¸ì‚¬íšŒ", "ê³¼í•™ê¸°ìˆ "], index=0, key="custom_major_field")
                
                if custom_major_field == "ì¸ë¬¸ì‚¬íšŒ":
                    custom_minor_field = st.selectbox("ì†Œë¶„ì•¼", ["ì¸ë¬¸", "ì˜ˆìˆ ", "ë²•", "ê²½ì œ"], index=0, key="custom_minor_field")
                else:  # ê³¼í•™ê¸°ìˆ 
                    custom_minor_field = st.selectbox("ì†Œë¶„ì•¼", ["ê³¼í•™", "ê¸°ìˆ "], index=0, key="custom_minor_field")
                
                custom_type = st.selectbox("ìœ í˜•", ["ë‹¨ì¼í˜•", "(ê°€), (ë‚˜) ë¶„ë¦¬í˜•"], index=0, key="custom_type")
                custom_topic = st.text_area("ì£¼ì œ", value="", height=100, 
                                          placeholder="ì—¬ê¸°ì— ì›í•˜ëŠ” ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.",
                                          key="custom_topic")
                
                # Custom íƒ­ ì‹¤í–‰ ë²„íŠ¼
                custom_run_button = st.button("ì§€ë¬¸ ìƒì„±", type="primary", use_container_width=True, key="custom_run")
        
        # Temperature ì„¹ì…˜ (í”„ë¡¬í”„íŠ¸ ì•„ë˜ë¡œ ì´ë™)
        with st.container(border=True):
            st.markdown("**Temperature**")
            st.markdown("ë‹¤ì–‘ì„±ì„ ì¡°ì ˆí•˜ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ë†’ì„ìˆ˜ë¡ ì§€ë¬¸ì˜ ë‚´ìš©ì´ ë‹¤ì±„ë¡­ì§€ë§Œ, ë‹¤ì†Œ ì‚°ë§Œí•˜ê³  ì¼ê´€ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n(ê¸°ë³¸ê°’: 0.85)")
            
            # Temperature ì´ˆê¸°ê°’ ì„¤ì •
            if "temperature" not in st.session_state:
                st.session_state.temperature = 0.85
                
            temperature = st.slider("", 0.5, 1.2, st.session_state.temperature, 0.05, key="temp_slider")
            
            # slider ê°’ì´ ë³€ê²½ë˜ë©´ session_state ì—…ë°ì´íŠ¸
            st.session_state.temperature = temperature

# ë‘ ë²ˆì§¸ ì»¬ëŸ¼: Reasoning & Expert Response
with col2:
    st.markdown("#### 2. ëª¨ë¸ ì‚¬ê³  ê³¼ì •")
    with st.container(border=True, height=container_height):
        reasoning_placeholder = st.empty()
        reasoning_placeholder.info("AI ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •ì´ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.")

# ì„¸ ë²ˆì§¸ ì»¬ëŸ¼: Final Response
with col3:
    st.markdown("#### 3. ìµœì¢… ì§€ë¬¸")
    final_placeholder = st.empty()
    # ì»¤ìŠ¤í…€ CSS ì»¨í…Œì´ë„ˆë¡œ ì´ˆê¸° ìƒíƒœ í‘œì‹œ
    final_placeholder.markdown('''
    <div class="final-response-container">
        <p style="color: #666; text-align: center; margin-top: 250px;">ìµœì¢… ì§€ë¬¸ì´ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.</p>
    </div>
    ''', unsafe_allow_html=True)


# --- ë„êµ¬ í•¨ìˆ˜ (ì „ë¬¸ê°€ í˜¸ì¶œ) ---
async def execute_request_for_expert(input_text: str) -> str:
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        return "[expert_error] Missing GOOGLE_API_KEY"

    def _call_sync() -> str:
        try:
            # Expert ëª¨ë¸ì€ í•­ìƒ Google API ì‚¬ìš© (Gemini)
            client = OpenAI(
                api_key=api_key,
                base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
            )
            resp = client.chat.completions.create(
                model=EXPERT_MODEL_NAME,
                messages=[
                    {"role": "system", "content": EXPERT_PROMPT},
                    {"role": "user", "content": input_text},
                ],
            )
            return resp.choices[0].message.content if resp.choices else ""
        except Exception as e:
            return f"[expert_error] {e}"

    result = await asyncio.to_thread(_call_sync)
    return result or "[expert_empty]"


# --- í…ìŠ¤íŠ¸ì—ì„œ expert íƒœê·¸ íŒŒì‹± í•¨ìˆ˜ ---
import re

def parse_expert_calls(text: str) -> tuple[str, list[str]]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ <expert>...</expert> ë˜ëŠ” <expertcall>...</expertcall> íƒœê·¸ë¥¼ ì°¾ì•„ì„œ íŒŒì‹±í•©ë‹ˆë‹¤.
    
    Returns:
        (cleaned_text, expert_calls): íƒœê·¸ê°€ ì œê±°ëœ í…ìŠ¤íŠ¸ì™€ ì „ë¬¸ê°€ í˜¸ì¶œ ë¦¬ìŠ¤íŠ¸
    """
    expert_calls = []
    
    # <expert>...</expert> íŒ¨í„´ ì°¾ê¸° 
    expert_pattern = r'<expert>(.*?)</expert>'
    expert_matches = re.findall(expert_pattern, text, re.DOTALL)
    
    # <expertcall>...</expertcall> íŒ¨í„´ ì°¾ê¸°
    expertcall_pattern = r'<expertcall>(.*?)</expertcall>'
    expertcall_matches = re.findall(expertcall_pattern, text, re.DOTALL)
    
    # ëª¨ë“  ë§¤ì¹­ ê²°ê³¼ë¥¼ í•©ì¹˜ê¸°
    for match in expert_matches:
        expert_calls.append(match.strip())
    
    for match in expertcall_matches:
        expert_calls.append(match.strip())
    
    # ë‘ íŒ¨í„´ ëª¨ë‘ ì œê±°í•œ í…ìŠ¤íŠ¸ ë°˜í™˜
    cleaned_text = re.sub(expert_pattern, '', text, flags=re.DOTALL)
    cleaned_text = re.sub(expertcall_pattern, '', cleaned_text, flags=re.DOTALL).strip()
    
    return cleaned_text, expert_calls

# --- ìƒˆë¡œìš´ ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜ (Vertex AI ì¡°ì •ëœ ëª¨ë¸ ê¸°ë°˜) ---
async def run_vertex_ai_flow_streaming(endpoint_id: str, project_id: str, location: str, system_prompt: str, user_prompt: str, temperature: float):
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    round_idx = 0
    final_text = ""

    while True:
        round_idx += 1
        if round_idx > 30:
            break

        try:
            # Vertex AI ì¡°ì •ëœ ëª¨ë¸ í˜¸ì¶œ
            content = await call_vertex_ai_endpoint(
                endpoint_id=endpoint_id,
                project_id=project_id,
                location=location,
                messages=messages,
                temperature=temperature
            )
            
            if not content or content.startswith("[error]"):
                yield {"type": "think", "content": f"Model error: {content}"}
                break
            
            # assistant ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            messages.append({"role": "assistant", "content": content})
            
            # <expert> íƒœê·¸ íŒŒì‹±
            cleaned_text, expert_calls = parse_expert_calls(content)
            
            # <passage> íƒœê·¸ í™•ì¸ - ìµœì¢… ì‘ë‹µ ì—¬ë¶€ íŒë‹¨
            has_passage = "<passage>" in cleaned_text and "</passage>" in cleaned_text
            
            if has_passage:
                # <passage> íƒœê·¸ì—ì„œ ìµœì¢… ì§€ë¬¸ ì¶”ì¶œ
                start_tag = "<passage>"
                end_tag = "</passage>"
                start_idx = cleaned_text.find(start_tag)
                end_idx = cleaned_text.find(end_tag)
                
                if start_idx != -1 and end_idx != -1:
                    # <passage> ì´ì „ ë¶€ë¶„ (thinking/reasoning)
                    before_passage = cleaned_text[:start_idx].strip()
                    # <passage> ë‚´ìš© (ìµœì¢… ì§€ë¬¸)
                    passage_content = cleaned_text[start_idx + len(start_tag):end_idx].strip()
                    
                    # ì´ì „ ë¶€ë¶„ì´ ìˆìœ¼ë©´ thinkingìœ¼ë¡œ ì¶œë ¥
                    if before_passage:
                        yield {"type": "think", "content": before_passage}
                    
                    remaining_text = passage_content
                    is_final_response = True
                else:
                    remaining_text = cleaned_text
                    is_final_response = False
            else:
                # <passage> íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                remaining_text = cleaned_text
                is_final_response = False
            
            # expert í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°
            if expert_calls:
                # expert í˜¸ì¶œ ì „ì— ì¤‘ê°„ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ thinkingìœ¼ë¡œ ì¶œë ¥
                if remaining_text and not has_passage:
                    yield {"type": "think", "content": remaining_text}
                
                for expert_input in expert_calls:
                    # ì „ë¬¸ê°€ ì§ˆì˜ ì‹œì‘ ì´ë²¤íŠ¸ (ì§ˆì˜ ë‚´ìš© ë¨¼ì € í‘œì‹œ)
                    yield {"type": "tool_start", "input": expert_input}
                    
                    # ì „ë¬¸ê°€ í•¨ìˆ˜ í˜¸ì¶œ (ì¼ë°˜ Gemini API ì‚¬ìš©)
                    expert_result = await execute_request_for_expert(expert_input)
                    
                    # user ë©”ì‹œì§€ë¡œ ì „ë¬¸ê°€ ê²°ê³¼ ì¶”ê°€
                    messages.append({
                        "role": "user",
                        "content": expert_result
                    })
                    
                    # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ë¬¸ê°€ ì‘ë‹µ í‘œì‹œ
                    yield {"type": "tool_output", "content": expert_result, "input": expert_input}
                
                # <passage> íƒœê·¸ê°€ ìˆìœ¼ë©´ ìµœì¢… ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì¢…ë£Œ
                if has_passage and remaining_text:
                    yield {"type": "final", "content": remaining_text}
                    break
                else:
                    # expert í˜¸ì¶œ í›„ ë‹¤ìŒ ë¼ìš´ë“œë¡œ ê³„ì†
                    continue
                    
            else:
                # expert í˜¸ì¶œì´ ì—†ëŠ” ê²½ìš°
                if has_passage and remaining_text:
                    # <passage> íƒœê·¸ê°€ ìˆìœ¼ë©´ ìµœì¢… ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì¢…ë£Œ
                    yield {"type": "final", "content": remaining_text}
                    break
                elif remaining_text:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ thinkingìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ê³„ì†
                    yield {"type": "think", "content": remaining_text}
                    continue
                else:
                    # ì•„ë¬´ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ ë¼ìš´ë“œë¡œ
                    continue
                
        except Exception as e:
            yield {"type": "think", "content": f"[error] {e}"}
            break

    # ìµœì¢… í…ìŠ¤íŠ¸ ê²°ì •
    if not final_text:
        for msg in reversed(messages):
            if msg.get("role") == "assistant" and msg.get("content"):
                final_text = msg["content"]
                break
    
    # í˜¹ì‹œ ë¹ˆ final response ë°©ì§€
    if not final_text:
        yield {"type": "final", "content": final_text or ""}



# --- íƒ€ì´í•‘ íš¨ê³¼ í•¨ìˆ˜ ---
async def typing_effect(text: str, placeholder, is_final: bool = False):
    """í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ íƒ€ì´í•‘ íš¨ê³¼ë¥¼ ë‚´ë©° í‘œì‹œ"""
    if not text:
        return
    
    # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í•  (í•œêµ­ì–´ì™€ ì˜ì–´ ëª¨ë‘ ê³ ë ¤)
    tokens = re.findall(r'\S+|\s+', text)
    
    displayed_text = ""
    for i, token in enumerate(tokens):
        displayed_text += token
        
        if is_final:
            # ìµœì¢… ì‘ë‹µì€ HTML í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
            final_response_content = f'<div class="final-response-container"><div class="passage-font-no-border">{format_text_to_html(displayed_text.strip())}</div></div>'
            placeholder.markdown(final_response_content, unsafe_allow_html=True)
        else:
            # ì‘ê°€ ëª¨ë¸ ì‚¬ê³ ê³¼ì •ì€ ì¼ë°˜ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í‘œì‹œ
            placeholder.markdown(displayed_text.strip() + " â–Š")  # ì»¤ì„œ ì¶”ê°€
        
        # íƒ€ì´í•‘ ì†ë„ ì¡°ì ˆ (í† í° ê¸¸ì´ì— ë”°ë¼ ì¡°ì ˆ)
        if len(token.strip()) > 0:  # ì‹¤ì œ ë‚´ìš©ì´ ìˆëŠ” í† í°ë§Œ
            await asyncio.sleep(0.01)  # 30ms ë”œë ˆì´
    
    # ìµœì¢…ì ìœ¼ë¡œ ì»¤ì„œ ì œê±°
    if not is_final:
        placeholder.markdown(displayed_text.strip())

# --- ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ë¡œì§ ---
async def stream_and_render(final_user_prompt: str, selected_system_prompt: str):
    # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
    st.session_state["is_streaming"] = True
    
    try:
        # ì‹œê°„ìˆœìœ¼ë¡œ ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ ì €ì¥
        all_events = []
        final_content = ""
        
        # Vertex AI ì„¤ì •ê°’ ì‚¬ìš©
        endpoint_id = ENDPOINT_ID
        project_id = PROJECT_ID
        location = LOCATION
        
        # ë™ì ìœ¼ë¡œ ì„¹ì…˜ì„ ì¶”ê°€í•  ë©”ì¸ ì»¨í…Œì´ë„ˆ
        with reasoning_placeholder.container():
            reasoning_main = st.container()
            
        expert_containers = {}  # ì „ë¬¸ê°€ ì§ˆì˜ë³„ ë©”ì¸ ì»¨í…Œì´ë„ˆ ì €ì¥
        
        async for event in run_vertex_ai_flow_streaming(
            endpoint_id=endpoint_id,
            project_id=project_id,
            location=location,
            system_prompt=selected_system_prompt,
            user_prompt=final_user_prompt,
            temperature=temperature,
        ):
            etype = event.get("type")
            
            if etype == "think":
                # ì‘ê°€ ëª¨ë¸ ì‚¬ê³  ê³¼ì • - íƒ€ì´í•‘ íš¨ê³¼ ì ìš©
                think_content = event.get("content", "").strip()
                if think_content:
                    with reasoning_main:
                        st.markdown("#### ì‘ê°€ ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •")
                        thinking_placeholder = st.empty()
                    
                    # íƒ€ì´í•‘ íš¨ê³¼ë¡œ í‘œì‹œ
                    await typing_effect(think_content, thinking_placeholder, is_final=False)
            
            elif etype == "tool_start":
                # ì „ë¬¸ê°€ ì§ˆì˜ ì‹œì‘ - ì§ˆì˜ ë‚´ìš©ë§Œ ë¨¼ì € í‘œì‹œ
                input_text = (event.get("input") or "").strip()
                
                with reasoning_main:
                    expert_section = st.container()
                    with expert_section:
                        st.markdown("#### ì „ë¬¸ê°€ ëª¨ë¸ì—ê²Œ ì§ˆì˜í•˜ê¸°")
                        
                        # ì§ˆì˜ ë‚´ìš©ë§Œ í‘œì‹œ
                        with st.expander("ì§ˆë¬¸ ë‚´ìš©", expanded=False):
                            st.markdown(input_text)
                    
                    # ì´ ì„¹ì…˜ì„ ì €ì¥í•´ë‘ì–´ì„œ ë‚˜ì¤‘ì— ì‘ë‹µì„ ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ í•¨
                    expert_containers[input_text] = expert_section
            
            elif etype == "tool_output":
                # ì „ë¬¸ê°€ ì‘ë‹µ ì™„ë£Œ - ì‘ë‹µ ìµìŠ¤íŒ¬ë”ë¥¼ ìƒˆë¡œ ìƒì„±
                out_text = (event.get("content") or "").strip()
                input_text = (event.get("input") or "").strip()
                
                # í•´ë‹¹ ì§ˆì˜ì— ëŒ€í•œ ì»¨í…Œì´ë„ˆ ì°¾ì•„ì„œ ì‘ë‹µ ì¶”ê°€
                if input_text in expert_containers:
                    with expert_containers[input_text]:
                        # ì‘ë‹µ ë‚´ìš© ìµìŠ¤íŒ¬ë”ë¥¼ ìƒˆë¡œ ìƒì„±
                        with st.expander("ì‘ë‹µ ë‚´ìš©", expanded=False):
                            st.markdown(out_text)
            
            elif etype == "final":
                # ìµœì¢… ì‘ë‹µ - íƒ€ì´í•‘ íš¨ê³¼ ì ìš©
                final_content = event.get("content", "").strip()
                if final_content:
                    await typing_effect(final_content, final_placeholder, is_final=True)
                break

    except Exception as e:
        st.error(f"ëª¨ë¸ í˜¸ì¶œ ë˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ
        st.session_state["is_streaming"] = False


# render_event í•¨ìˆ˜ ì œê±°ë¨ - ìƒˆë¡œìš´ 3ì»¬ëŸ¼ ë Œë”ë§ ë°©ì‹ ì‚¬ìš©


# ì‹¤í–‰ ë¡œì§ - Preset ë²„íŠ¼ í´ë¦­ ì‹œ
if 'preset_run_button' in locals() and preset_run_button:
    if 'system_prompt' in locals() and 'user_prompt' in locals() and system_prompt and user_prompt:
        final_user_prompt = user_prompt  # ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        selected_system_prompt = system_prompt
        # ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜ ì‹¤í–‰
        asyncio.run(stream_and_render(final_user_prompt, selected_system_prompt))
    else:
        st.error("Preset ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ì‹¤í–‰ ë¡œì§ - Custom ë²„íŠ¼ í´ë¦­ ì‹œ
if 'custom_run_button' in locals() and custom_run_button:
    custom_topic_content = st.session_state.get("custom_topic", "").strip()
    
    if custom_topic_content:
        # Custom íƒ­ ë°ì´í„° ì‚¬ìš©
        custom_major_field_content = st.session_state.get("custom_major_field", "ì¸ë¬¸ì‚¬íšŒ")
        custom_minor_field_content = st.session_state.get("custom_minor_field", "ì¸ë¬¸")
        custom_field_content = f"{custom_major_field_content} ({custom_minor_field_content})"
        custom_type_content = st.session_state.get("custom_type", "ë‹¨ì¼í˜•")
        
        final_user_prompt = format_prompt_from_components(custom_field_content, custom_type_content, custom_topic_content)
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì²« ë²ˆì§¸ ìƒ˜í”Œì—ì„œ ì¶”ì¶œ)
        default_system_prompt, _, _ = load_sample(0)
        selected_system_prompt = default_system_prompt if default_system_prompt else ""
        
        # ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜ ì‹¤í–‰
        asyncio.run(stream_and_render(final_user_prompt, selected_system_prompt))
    else:
        st.error("ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
