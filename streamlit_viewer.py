import streamlit as st
import asyncio
import json
from openai import AsyncOpenAI, OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

if not os.getenv("GOOGLE_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
if not os.getenv("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

from transformers import AutoTokenizer

# --- í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(layout="wide", page_title="ëª¨ë¸ ì§€ë¬¸ ìƒì„± ë·°ì–´")

# --- ì‚¬ìš©ì ì •ì˜ CSS ---
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Nanum+Myeongjo:wght@400;700;800&display=swap');

h3 {
    font-family: 'Nanum Myeongjo', serif !important;
    font-weight: 700;
}
.passage-container {
    height: 80vh;
    overflow-y: auto;
    border: 1px solid #e0e0e0;
    border-radius: 5px;
    padding: 15px;
}
.passage-font {
    font-family: 'Nanum Myeongjo', serif !important;
    line-height: 1.7;
    letter-spacing: -0.01em;
    font-weight: 500;
}
.passage-font p {
    text-indent: 1em;
    margin-bottom: 0em;
    margin-top: 0.5em;
}
</style>
""", unsafe_allow_html=True)


# --- ì„¤ì •ê°’ ---
DATASET_PATH = "GPT-sft-09-03-val.jsonl"
# OpenAI fine-tuned model ê¸°ë³¸ê°’ (í•„ìš” ì‹œ ì‚¬ì´ë“œë°”ì—ì„œ ë³€ê²½)
MODEL_NAME = "ft:gpt-4.1-2025-04-14:ksat-agent:exp-09-03-large:CBdqVMpr"


# --- ë„ìš°ë¯¸ í•¨ìˆ˜ ---
@st.cache_data
def get_dataset_info(path):
    """ë°ì´í„°ì…‹ íŒŒì¼ì˜ ì´ ìƒ˜í”Œ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        with open(path, "r", encoding='utf-8') as f:
            return len(f.readlines())
    except FileNotFoundError:
        return 0

@st.cache_data
def load_sample(index):
    """ì§€ì •ëœ ì¸ë±ìŠ¤ì˜ ë°ì´í„°ì…‹ ìƒ˜í”Œì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(DATASET_PATH, "r", encoding='utf-8') as f:
            line = f.readlines()[index]
            data = json.loads(line)
            
            system_prompt = data["messages"][0]["content"]
            user_prompt = data["messages"][1]["content"]
            expected_response_raw = data["messages"][-1]["content"]
            
            if "<think>" in expected_response_raw:
                expected_response = expected_response_raw.split("</think>", 1)[1].strip()
            else:
                expected_response = expected_response_raw.strip()
            
            return system_prompt, user_prompt, expected_response
    except Exception as e:
        st.error(f"ë°ì´í„°ì…‹ ë¡œë”© ì˜¤ë¥˜: {e}")
        return None, None, None

def format_text_to_html(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ ì¤„ë°”ê¿ˆì„ HTML ë‹¨ë½(<p>)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    paragraphs = text.strip().split('\n')
    html_paragraphs = [f"<p>{p.strip()}</p>" for p in paragraphs if p.strip()]
    return "".join(html_paragraphs)


# --- ì‚¬ì´ë“œë°” UI ---
st.sidebar.title("âš™ï¸ ëª¨ë¸ ë° ë°ì´í„° ì„¤ì •")

total_samples = get_dataset_info(DATASET_PATH)
if total_samples > 0:
    dataset_index = st.sidebar.selectbox(
        "ë°ì´í„° ìƒ˜í”Œ ì„ íƒ",
        options=range(total_samples),
        format_func=lambda x: f"Sample #{x}",
        index=0
    )
else:
    st.sidebar.error("ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `DATASET_PATH`ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

model_name = st.sidebar.text_input("ëª¨ë¸ ì´ë¦„ (OpenAI)", value=MODEL_NAME)
tokenizer_path = st.sidebar.text_input("í† í¬ë‚˜ì´ì € ê²½ë¡œ (ì˜µì…˜)", value=model_name)


st.sidebar.divider()
st.sidebar.subheader("ìƒì„± íŒŒë¼ë¯¸í„°")
temperature = st.sidebar.slider("Temperature", 0.0, 1.5, 0.8, 0.05)
top_p = st.sidebar.slider("Top P", 0.0, 1.0, 0.95, 0.05)
presence_penalty = st.sidebar.slider("Presence Penalty", -1.0, 1.0, 0.0, 0.05)

run_button = st.sidebar.button("ğŸš€ ì§€ë¬¸ ìƒì„± ì‹œì‘", type="primary")


# --- ë©”ì¸ íŒ¨ë„ UI ---
col1, col2 = st.columns(2)
system_prompt, user_prompt, expected_response = load_sample(dataset_index)

with col1:
    st.subheader("ì›ë³¸ ì§€ë¬¸")
    with st.container(border=True, height=600):
        if expected_response:
            html_expected = f'<div class="passage-font">{format_text_to_html(expected_response)}</div>'
            st.markdown(html_expected, unsafe_allow_html=True)
        else:
            st.warning("ì„ íƒëœ ì¸ë±ìŠ¤ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

with col2:
    st.subheader("ëª¨ë¸ ì‘ë‹µ")
    # ìŠ¤íŠ¸ë¦¼ë¦¿ ì»¨í…Œì´ë„ˆì— ë³´ë” ì ìš©
    with st.container(border=True, height=600):
        placeholder = st.empty()
        # ì´ˆê¸° ì•ˆë‚´ ë©”ì‹œì§€
        with placeholder.container():
            st.markdown(f'<div class="passage-font">ì™¼ìª½ ì„¤ì • íŒ¨ë„ì—ì„œ "ì§€ë¬¸ ìƒì„± ì‹œì‘" ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.</div>', unsafe_allow_html=True)


# --- ë„êµ¬ í•¨ìˆ˜ (ì „ë¬¸ê°€ í˜¸ì¶œ) ---
async def execute_request_for_expert(input_text: str) -> str:
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        return "[expert_error] Missing GOOGLE_API_KEY"

    def _call_sync() -> str:
        try:
            client = OpenAI(
                api_key=api_key,
                base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
            )
            resp = client.chat.completions.create(
                model="gemini-2.5-flash-lite",
                messages=[{"role": "user", "content": input_text}],
            )
            return resp.choices[0].message.content if resp.choices else ""
        except Exception as e:
            return f"[expert_error] {e}"

    result = await asyncio.to_thread(_call_sync)
    return result or "[expert_empty]"


# --- think/call ë£¨í”„ (ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ìƒì„±) ---
async def run_tool_call_flow_streaming(model_name: str, system_prompt: str, user_prompt: str, temperature: float, top_p: float, presence_penalty: float):
    client = AsyncOpenAI()

    tools = [
        {
            "type": "function",
            "function": {
                "name": "request_for_expert",
                "description": "Request information from an expert assistant and return textual content.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "input": {
                            "type": "string",
                            "description": "Instruction or query text for the expert LLM.",
                        }
                    },
                    "required": ["input"],
                    "additionalProperties": False,
                },
                "strict": True,
            }
        }
    ]

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    round_idx = 0
    turn_type = "think"
    final_text = ""

    while True:
        round_idx += 1
        if round_idx > 30:
            break

        if turn_type == "think":
            try:
                response = await client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    temperature=temperature,
                    top_p=top_p,
                    presence_penalty=presence_penalty,
                    tools=tools,
                    tool_choice="none",
                )
                content = response.choices[0].message.content or ""
                if content:
                    # ìµœì¢… ì‘ë‹µ íŒë‹¨: </think> ì´í›„ í…ìŠ¤íŠ¸ê°€ ìˆê³  ë„êµ¬ í˜¸ì¶œì´ ì•„ë‹Œ ê²½ìš°
                    if "</think>" in content:
                        before, after = content.split("</think>", 1)
                        reasoning_text = before
                        if "<think>" in reasoning_text:
                            reasoning_text = reasoning_text.split("<think>", 1)[1]
                        if reasoning_text.strip():
                            yield {"type": "think", "content": reasoning_text.strip()}
                        
                        after_text = after.strip()
                        # ë„êµ¬ í˜¸ì¶œì´ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                        if after_text and not after_text.startswith("<tool"):
                            # ì¤‘ìš”: assistant ë©”ì‹œì§€ë¥¼ ë¨¼ì € ì¶”ê°€í•œ í›„ ìµœì¢… ì‘ë‹µ ì²˜ë¦¬
                            messages.append({"role": "assistant", "content": content})
                            yield {"type": "final", "content": after_text}
                            break
                        else:
                            # </think> ì´í›„ ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°
                            messages.append({"role": "assistant", "content": content})
                            turn_type = "call"
                            continue
                    else:
                        yield {"type": "think", "content": content}
                        messages.append({"role": "assistant", "content": content})
                        turn_type = "call"
                        continue
                turn_type = "call"
            except Exception as e:
                yield {"type": "think", "content": f"[error - think] {e}"}
                break

        else:  # call turn
            try:
                response = await client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    temperature=temperature,
                    top_p=top_p,
                    presence_penalty=presence_penalty,
                    tools=tools,
                    tool_choice={"type": "function", "function": {"name": "request_for_expert"}},
                )

                assistant_message = response.choices[0].message
                content = assistant_message.content or ""
                tool_calls = assistant_message.tool_calls or []

                # ê°„í—ì  í…ìŠ¤íŠ¸ëŠ” reasoningì— í•©ë¥˜í•˜ê±°ë‚˜ ìµœì¢… ì‘ë‹µ íŒë‹¨ (ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬ í›„ ì¢…ë£Œ)
                has_final_response = False
                final_response_text = ""
                
                if content:
                    if "</think>" in content:
                        before, after = content.split("</think>", 1)
                        reasoning_text = before
                        if "<think>" in reasoning_text:
                            reasoning_text = reasoning_text.split("<think>", 1)[1]
                        if reasoning_text.strip():
                            yield {"type": "think", "content": reasoning_text.strip()}
                        
                        after_text = after.strip()
                        # ë„êµ¬ í˜¸ì¶œì´ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸ (ë°”ë¡œ breakí•˜ì§€ ì•ŠìŒ)
                        if after_text and not after_text.startswith("<tool"):
                            has_final_response = True
                            final_response_text = after_text
                    else:
                        yield {"type": "think", "content": content}

                msg_to_add = {"role": "assistant", "content": content}
                if tool_calls:
                    msg_to_add["tool_calls"] = [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {"name": tc.function.name, "arguments": tc.function.arguments},
                        }
                        for tc in tool_calls
                    ]
                messages.append(msg_to_add)

                if tool_calls:
                    for tool_call in tool_calls:
                        try:
                            args = json.loads(tool_call.function.arguments)
                        except Exception:
                            args = {"input": tool_call.function.arguments}
                        # ë„êµ¬ í˜¸ì¶œ ì§ˆë¬¸ë„ í•¨ê»˜ ì „ë‹¬
                        tool_input = args.get("input", "")
                        tool_output_content = await execute_request_for_expert(tool_input)
                        messages.append({
                            "tool_call_id": tool_call.id,
                            "role": "tool",
                            "name": tool_call.function.name,
                            "content": tool_output_content,
                        })

                        yield {"type": "tool_output", "content": tool_output_content, "input": tool_input}

                    # ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬ í›„ ìµœì¢… ì‘ë‹µì´ ìˆë‹¤ë©´ ì¶œë ¥í•˜ê³  ì¢…ë£Œ
                    if has_final_response:
                        yield {"type": "final", "content": final_response_text}
                        break
                    else:
                        turn_type = "think"  # ë‹¤ìŒ ì‚¬ê³  í„´ìœ¼ë¡œ
                else:
                    # ë„êµ¬ í˜¸ì¶œì´ ì—†ëŠ” ê²½ìš°
                    if has_final_response:
                        yield {"type": "final", "content": final_response_text}
                        break
                    elif content:
                        # ìµœì¢… ì‘ë‹µë„ ì•„ë‹ˆê³  ë„êµ¬ í˜¸ì¶œë„ ì—†ìœ¼ë©´ ì¢…ë£Œ
                        yield {"type": "final", "content": content}
                        break
                    else:
                        break
            except Exception as e:
                yield {"type": "think", "content": f"[error - call] {e}"}
                break

    # ìµœì¢… í…ìŠ¤íŠ¸ ê²°ì •
    if not final_text:
        for msg in reversed(messages):
            if msg.get("role") == "assistant" and msg.get("content"):
                final_text = msg["content"]
                break
    yield {"type": "final", "content": final_text or ""}


# --- ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ë¡œì§ ---
async def stream_and_render():
    try:
        # í† í¬ë‚˜ì´ì €ëŠ” ì„ íƒ ì‚¬í•­ (ê¸¸ì´ ê³„ì‚° ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ë¬´ì‹œ)
        try:
            _ = AutoTokenizer.from_pretrained(tokenizer_path)
        except Exception:
            pass

        # ê° ì´ë²¤íŠ¸ë§ˆë‹¤ ìƒˆë¡œìš´ ì»¨í…Œì´ë„ˆë¥¼ ìƒì„±í•´ì„œ ë³µì œ ë¬¸ì œ ë°©ì§€
        event_containers = []
        
        async for event in run_tool_call_flow_streaming(
            model_name=model_name,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=temperature,
            top_p=top_p,
            presence_penalty=presence_penalty,
        ):
            etype = event.get("type")
            
            # ê¸°ì¡´ placeholderë¥¼ ë¹„ìš°ê³  ìƒˆë¡œìš´ ì»¨í…Œì´ë„ˆ ìƒì„±
            with placeholder.container():
                # ì´ì „ ì´ë²¤íŠ¸ë“¤ì„ ë‹¤ì‹œ ë Œë”ë§
                for prev_event in event_containers:
                    render_event(prev_event)
                
                # í˜„ì¬ ì´ë²¤íŠ¸ ë Œë”ë§
                render_event(event)
                
                # ì´ë²¤íŠ¸ ì €ì¥
                event_containers.append(event)
                
                if etype == "final":
                    break

    except Exception as e:
        st.error(f"ëª¨ë¸ í˜¸ì¶œ ë˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def render_event(event):
    """ê°œë³„ ì´ë²¤íŠ¸ë¥¼ ë Œë”ë§í•˜ëŠ” í•¨ìˆ˜"""
    etype = event.get("type")
    
    if etype == "think":
        st.markdown("### ğŸ¤” Reasoning:")
        st.markdown(f'<div class="passage-font">{format_text_to_html((event.get("content") or "").strip())}</div>', unsafe_allow_html=True)
        
    elif etype == "tool_output":
        out_text = (event.get("content") or "").strip()
        input_text = (event.get("input") or "").strip()
        st.markdown("### ğŸ” Request for Expert")
        with st.expander("ğŸ“¤ Question to Expert", expanded=False):
            st.markdown(input_text)
        with st.expander("ğŸ§  Expert's response", expanded=False):
            st.markdown(out_text)

    elif etype == "final":
        st.markdown("### âœ… Final Response:")
        st.markdown(f'<div class="passage-font">{format_text_to_html((event.get("content") or "").strip())}</div>', unsafe_allow_html=True)


if run_button:
    # ë²„íŠ¼ì´ í´ë¦­ë˜ë©´ ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜ë¥¼ ì‹¤í–‰
    asyncio.run(stream_and_render())
