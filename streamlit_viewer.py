import streamlit as st
import asyncio
import json
from openai import AsyncOpenAI, OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

if not os.getenv("GOOGLE_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
if not os.getenv("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

from transformers import AutoTokenizer

# --- í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(layout="wide", page_title="ëª¨ë¸ ì§€ë¬¸ ìƒì„± ë·°ì–´")

# --- ì‚¬ìš©ì ì •ì˜ CSS ---
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Nanum+Myeongjo:wght@400;700;800&display=swap');

h3 {
    font-family: 'Nanum Myeongjo', serif !important;
    font-weight: 700;
}
.passage-container {
    height: 80vh;
    overflow-y: auto;
    border: 1px solid #e0e0e0;
    border-radius: 5px;
    padding: 15px;
}
.passage-font {
    font-family: 'Nanum Myeongjo', serif !important;
    line-height: 1.7;
    letter-spacing: -0.01em;
    font-weight: 500;
}
.passage-font p {
    text-indent: 1em;
    margin-bottom: 0em;
    margin-top: 0.5em;
}
</style>
""", unsafe_allow_html=True)


# --- ì„¤ì •ê°’ ---
DATASET_PATH = "GPT-sft-09-06-val.jsonl"
# OpenAI fine-tuned model ê¸°ë³¸ê°’ (í•„ìš” ì‹œ ì‚¬ì´ë“œë°”ì—ì„œ ë³€ê²½)
#MODEL_NAME = "ft:gpt-4.1-2025-04-14:ksat-agent:ksat-exp-09-05-large:CCIkLCNp"
MODEL_NAME = "ft:gpt-4.1-2025-04-14:ksat-agent:ksat-exp-09-06-large:CCMOwou1"
EXPERT_MODEL_NAME = "gemini-2.5-flash"

EXPERT_PROMPT = """
ë‹¹ì‹ ì€ ì‘ê°€ ëª¨ë¸ì—ê²Œ ìˆ˜ëŠ¥ ì§€ë¬¸ì„ ì‘ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ ëª¨ë¸ì…ë‹ˆë‹¤.
ì‘ê°€ ëª¨ë¸ì´ ìš”ì²­í•˜ëŠ” ì •ë³´ë¥¼ ì•„ë˜ì˜ ì§€ì¹¨ì— ë”°ë¼ ì œê³µí•´ ì£¼ì„¸ìš”.
1. íŠ¹ì • ê°œë…ì´ë‚˜ ì¸ë¬¼ì˜ ì£¼ì¥ì„ ì—¬ëŸ¬ ê°œ ì œì‹œí•  ë•Œì—ëŠ” ë²™ë ¬ì ìœ¼ë¡œ ë‚˜ì—´í•˜ì§€ ë§ê³  ê° ê°œë… ë˜ëŠ” ì£¼ì¥ì˜ ê³µí†µì  ë° ì°¨ì´ì ì´ ë°œìƒí•˜ëŠ” ëŒ€ë¦½ì ì„ ëª…í™•íˆ í•˜ì—¬ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.
2. ê³¼í•™ì /ê²½ì œì  ì›ë¦¬ ë˜ëŠ” ê¸°ìˆ ì˜ ì‘ë™ ì›ë¦¬ë¥¼ ì œì‹œí•  ë•Œì—ëŠ” ì›ë¦¬ë¥¼ í”¼ìƒì /ê´‘ë²”ìœ„í•˜ê²Œ ë‚˜ì—´í•˜ê¸° ë§ê³ , ë¯¸ì‹œì ì´ê³  ê¹Šì´ ìˆê²Œ ì„¤ëª…í•˜ì—¬ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.
3. ë²•ì  ê·œì •ì´ë‚˜ ì œë„ì˜ ì‘ë™ ì›ë¦¬ë¥¼ ì œì‹œí•  ë•Œì—ëŠ” ê·œì •ì´ë‚˜ ì œë„ë¥¼ ì¤„ì¤„ì´ ë‚˜ì—´í•˜ê¸°ë³´ë‹¤ëŠ”, í•´ë‹¹ ê·œì •ì´ ë“±ì¥í•œ ë°°ê²½(ë˜ëŠ” í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œ)ê³¼ ê·¸ ê·œì •ì˜ ì˜ì˜ë¥¼ ì„¤ëª…í•˜ê³ , í•´ë‹¹ ê·œì •ì´ ì ìš©ë˜ëŠ” ì¡°ê±´ì„ ì¼ìƒì  ì‚¬ë¡€ë¥¼ ë“¤ì–´ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
4. ì‘ê°€ ëª¨ë¸ì´ ë‹¤ì†Œ ê´‘ë²”ìœ„í•œ ì£¼ì œì— ëŒ€í•´ ì§ˆë¬¸í•œë‹¤ë©´, ì‘ê°€ ëª¨ë¸ì´ ì§€ë¬¸ì— í¬í•¨í•  ë‚´ìš©ì„ íƒìƒ‰í•˜ê³  ìˆëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒì˜ ì„œì‚¬ êµ¬ì¡° ì¤‘ í•˜ë‚˜ë¥¼ ì‚´ë ¤ ì…ì²´ì ìœ¼ë¡œ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.
    - ë¬¸ì œ ë°œìƒ ë° í•´ê²° êµ¬ì¡° : ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ì›ë¦¬ì™€ ê·¸ ì›ë¦¬ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ìˆ˜ë‹¨ê³¼ ë°©ë²•ì„ ì œì‹œí•˜ê³ , ê·¸ ì›ë¦¬ë¥¼ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤. ì¶”ê°€ë¡œ í•´ê²°ë²•ì˜ í•œê³„ì™€ ê·¸ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ë‹¤ë¥¸ ë°©ë²•ì„ ì œì‹œí•˜ëŠ” ê²ƒë„ ë°”ëŒì§í•©ë‹ˆë‹¤.
    - ë‹¤ì–‘í•œ ê²¬í•´ ë¹„êµ êµ¬ì¡° : í•˜ë‚˜ì˜ í™”ì œì— ëŒ€í•œ ì—¬ëŸ¬ ì¸ë¬¼ì´ë‚˜ í•™íŒŒì˜ ê´€ì ì„ ìˆœì°¨ì ìœ¼ë¡œ ì œì‹œí•˜ê³ , ê·¸ë“¤ì˜ í•´ì„ê³¼ ì£¼ì¥ì˜ ì°¨ì´ì , ë•Œë¡œëŠ” ì„œë¡œì˜ ê²¬í•´ì— ëŒ€í•œ ë¹„íŒê³¼ ê·¸ì— ëŒ€í•œ ë°˜ë°•ì„ ëª…í™•í•˜ê²Œ ë“œëŸ¬ëƒ…ë‹ˆë‹¤. ì´ êµ¬ì¡°ëŠ” ê° ê²¬í•´ì˜ í•µì‹¬ ë‚´ìš©ì„ ì •í™•íˆ íŒŒì•…í•˜ê³  ì„œë¡œ ë¹„êµ/ëŒ€ì¡°í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
    - ê°œë…(ë˜ëŠ” ì¡°ê±´) ë° ì ìš© ì‚¬ë¡€ êµ¬ì¡° : íŠ¹ì • ê°œë…ì´ë‚˜ ì œë„ë¥¼ ì •ì˜í•œ ë’¤, ì´ì™€ ê´€ë ¨ëœ ë²•ë¥ ì´ë‚˜ ê·œì¹™ì˜ êµ¬ì²´ì ì¸ ì¡°í•­ê³¼ ì¡°ê±´ì„ ìƒì„¸íˆ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ í•´ë‹¹ êµ¬ì¡°ê°€ ì ìš©ë  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ì œì‹œí•´ë„ ì¢‹ìŠµë‹ˆë‹¤. ì´ êµ¬ì¡°ëŠ” ì§€ë¬¸ì˜ ì •ë³´ë¥¼ êµ¬ì²´ì ì¸ ì‚¬ë¡€ì— ì ìš©í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
5. ìˆ˜ëŠ¥ì€ ë°°ê²½ì§€ì‹ì´ ì•„ë‹Œ ë…¼ë¦¬ì  ê·œì¹™ì„ ì´í•´í•˜ê³  ì ìš©í•˜ëŠ” ì‹œí—˜ì…ë‹ˆë‹¤. ê°œë…ì˜ ì–‘ê³¼ ë‹¤ì–‘ì„±ë³´ë‹¤ëŠ” ì¡°ê±´, ì¸ê³¼, ëŒ€ë¦½, ë¶„ê¸°, ìœ„ê³„ ë“± ë…¼ë¦¬ì  ê·œì¹™ì„ ëª…í™•íˆ í•˜ì—¬ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.
6. ìˆ˜ì‹ì„ í†µí•œ ì„¤ëª…ë³´ë‹¤ëŠ”, ì–¸ì–´ë¥¼ í™œìš©í•˜ì—¬ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
7. ë°°ê²½ ì§€ì‹ ìˆ˜ì¤€ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³ ë ¤í•˜ì—¬ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.
    - ìˆ˜í•™: ì‚¬ì¹™ì—°ì‚°ê³¼ ê±°ë“­ì œê³± ì •ë„ì˜ ê¸°ì´ˆì ì¸ ìˆ˜í•™ ì§€ì‹ë§Œì„ ê°–ì¶˜ ë…ìë¥¼ ì „ì œë¡œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ ì´ìƒì˜ ìˆ˜í•™ì  ì›ë¦¬ë¥¼ ì„¤ëª…í•  ë• ìˆ˜ì‹ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ê°€ê¸‰ì  ì–¸ì–´ë¡œ í’€ì–´ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.
    - ê³¼í•™: í˜, ì†ë„, ê±°ë¦¬, ë¶„ì, ì›ì, ë°”ì´ëŸ¬ìŠ¤, ë¯¸ìƒë¬¼ ë“± ê¸°ì´ˆì ì¸ ê°œë…ë§Œì„ ê°–ì¶˜ ë…ìë¥¼ ì „ì œí•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ ì´ìƒì˜ ê°œë…ì„ ë„ì…í•´ì•¼ í•  ê²½ìš°, ëª…í™•í•œ ì •ì˜ì™€ í•¨ê»˜ êµ¬ì²´ì ì¸ ê°œë… ì„¤ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤.
    - ì‚¬íšŒ: í™”í, ê²½ê¸°, í†µí™”, ì±„ê¶Œ, ë¯¼ë²•, êµ­íšŒ, ë³´ë„ ë“± ê¸°ì´ˆì ì¸ ì‚¬íšŒ ìš©ì–´ë¥¼ ìˆ™ì§€í•œ ë…ìë¥¼ ì „ì œí•©ë‹ˆë‹¤. ê·¸ ì´ìƒì˜ ê°œë…ì„ ë„ì…í•´ì•¼ í•  ê²½ìš°, ëª…í™•í•œ ì •ì˜ì™€ í•¨ê»˜ êµ¬ì²´ì ì¸ ê°œë… ì„¤ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤.
8. í•´ë‹¹ ë„ë©”ì¸ì˜ ì „ë¬¸/íŠ¹ìˆ˜ ìš©ì–´ ëŒ€ì‹ , ì¼ìƒì ì¸ ìš©ì–´ì™€ ì‚¬ë¡€ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.
9. ì •ë³´ëŠ” ìš”ì²­ì— ì¶©ì‹¤í•˜ë˜ ìµœëŒ€í•œ ì§§ê³  ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”. ìš”ì²­í•œ ë²”ìœ„ ì™¸ì˜ ì •ë³´ëŠ” ì œê³µí•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
"""

# --- ë„ìš°ë¯¸ í•¨ìˆ˜ ---
@st.cache_data
def get_dataset_info(path):
    """ë°ì´í„°ì…‹ íŒŒì¼ì˜ ì´ ìƒ˜í”Œ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        with open(path, "r", encoding='utf-8') as f:
            return len(f.readlines())
    except FileNotFoundError:
        return 0

@st.cache_data
def load_sample(index):
    """ì§€ì •ëœ ì¸ë±ìŠ¤ì˜ ë°ì´í„°ì…‹ ìƒ˜í”Œì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(DATASET_PATH, "r", encoding='utf-8') as f:
            line = f.readlines()[index]
            data = json.loads(line)
            
            system_prompt = data["messages"][0]["content"]
            user_prompt = data["messages"][1]["content"]
            expected_response_raw = data["messages"][-1]["content"]
            
            if "<think>" in expected_response_raw:
                expected_response = expected_response_raw.split("</think>", 1)[1].strip()
            else:
                expected_response = expected_response_raw.strip()
            
            return system_prompt, user_prompt, expected_response
    except Exception as e:
        st.error(f"ë°ì´í„°ì…‹ ë¡œë”© ì˜¤ë¥˜: {e}")
        return None, None, None

def format_text_to_html(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ ì¤„ë°”ê¿ˆì„ HTML ë‹¨ë½(<p>)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    paragraphs = text.strip().split('\n')
    html_paragraphs = [f"<p>{p.strip()}</p>" for p in paragraphs if p.strip()]
    return "".join(html_paragraphs)


# --- ì‚¬ì´ë“œë°” UI ---
st.sidebar.title("âš™ï¸ ëª¨ë¸ ë° ë°ì´í„° ì„¤ì •")

total_samples = get_dataset_info(DATASET_PATH)
if total_samples > 0:
    dataset_index = st.sidebar.selectbox(
        "ë°ì´í„° ìƒ˜í”Œ ì„ íƒ",
        options=range(total_samples),
        format_func=lambda x: f"Sample #{x}",
        index=0
    )
else:
    st.sidebar.error("ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `DATASET_PATH`ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

model_name = st.sidebar.text_input("ëª¨ë¸ ì´ë¦„ (OpenAI)", value=MODEL_NAME)
tokenizer_path = st.sidebar.text_input("í† í¬ë‚˜ì´ì € ê²½ë¡œ (ì˜µì…˜)", value=model_name)


st.sidebar.divider()
st.sidebar.subheader("ìƒì„± íŒŒë¼ë¯¸í„°")
temperature = st.sidebar.slider("Temperature", 0.0, 1.5, 0.7, 0.05)
top_p = st.sidebar.slider("Top P", 0.0, 1.0, 1.0, 0.05)
presence_penalty = st.sidebar.slider("Presence Penalty", -1.0, 1.0, 0.1, 0.05)

run_button = st.sidebar.button("ğŸš€ ì§€ë¬¸ ìƒì„± ì‹œì‘", type="primary")


# --- ë©”ì¸ íŒ¨ë„ UI ---
col1, col2 = st.columns(2)
system_prompt, user_prompt, expected_response = load_sample(dataset_index)

with col1:
    st.subheader("ì›ë³¸ ì§€ë¬¸")
    with st.container(border=True, height=600):
        if expected_response:
            html_expected = f'<div class="passage-font">{format_text_to_html(expected_response)}</div>'
            st.markdown(html_expected, unsafe_allow_html=True)
        else:
            st.warning("ì„ íƒëœ ì¸ë±ìŠ¤ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

with col2:
    st.subheader("ëª¨ë¸ ì‘ë‹µ")
    # ìŠ¤íŠ¸ë¦¼ë¦¿ ì»¨í…Œì´ë„ˆì— ë³´ë” ì ìš©
    with st.container(border=True, height=600):
        placeholder = st.empty()
        # ì´ˆê¸° ì•ˆë‚´ ë©”ì‹œì§€
        with placeholder.container():
            st.markdown(f'<div class="passage-font">ì™¼ìª½ ì„¤ì • íŒ¨ë„ì—ì„œ "ì§€ë¬¸ ìƒì„± ì‹œì‘" ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.</div>', unsafe_allow_html=True)


# --- ë„êµ¬ í•¨ìˆ˜ (ì „ë¬¸ê°€ í˜¸ì¶œ) ---
async def execute_request_for_expert(input_text: str) -> str:
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        return "[expert_error] Missing GOOGLE_API_KEY"

    def _call_sync() -> str:
        try:
            client = OpenAI(
                api_key=api_key,
                base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
            )
            resp = client.chat.completions.create(
                model=EXPERT_MODEL_NAME,
                messages=[
                    {"role": "system", "content": EXPERT_PROMPT},
                    {"role": "user", "content": input_text},
                ],
            )
            return resp.choices[0].message.content if resp.choices else ""
        except Exception as e:
            return f"[expert_error] {e}"

    result = await asyncio.to_thread(_call_sync)
    return result or "[expert_empty]"


# --- think/call ë£¨í”„ (ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ìƒì„±) ---
async def run_tool_call_flow_streaming(model_name: str, system_prompt: str, user_prompt: str, temperature: float, top_p: float, presence_penalty: float):
    client = AsyncOpenAI()

    tools = [
        {
            "type": "function",
            "function": {
                "name": "request_for_expert",
                "description": "Request information from an expert assistant and return textual content.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "input": {
                            "type": "string",
                            "description": "Instruction or query text for the expert LLM.",
                        }
                    },
                    "required": ["input"],
                    "additionalProperties": False,
                },
                "strict": True,
            }
        }
    ]

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    round_idx = 0
    turn_type = "think"
    final_text = ""

    while True:
        round_idx += 1
        if round_idx > 30:
            break

        if turn_type == "think":
            try:
                response = await client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    temperature=temperature,
                    top_p=top_p,
                    presence_penalty=presence_penalty,
                    tools=tools,
                    tool_choice="none",
                )
                content = response.choices[0].message.content or ""
                if content:
                    # ìµœì¢… ì‘ë‹µ íŒë‹¨: </think> ì´í›„ í…ìŠ¤íŠ¸ê°€ ìˆê³  ë„êµ¬ í˜¸ì¶œì´ ì•„ë‹Œ ê²½ìš°
                    if "</think>" in content:
                        before, after = content.split("</think>", 1)
                        reasoning_text = before
                        if "<think>" in reasoning_text:
                            reasoning_text = reasoning_text.split("<think>", 1)[1]
                        if reasoning_text.strip():
                            yield {"type": "think", "content": reasoning_text.strip()}
                        
                        after_text = after.strip()
                        # ë„êµ¬ í˜¸ì¶œì´ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                        if after_text and not after_text.startswith("<tool"):
                            # ì¤‘ìš”: assistant ë©”ì‹œì§€ë¥¼ ë¨¼ì € ì¶”ê°€í•œ í›„ ìµœì¢… ì‘ë‹µ ì²˜ë¦¬
                            messages.append({"role": "assistant", "content": content})
                            yield {"type": "final", "content": after_text}
                            break
                        else:
                            # </think> ì´í›„ ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°
                            messages.append({"role": "assistant", "content": content})
                            turn_type = "call"
                            continue
                    else:
                        yield {"type": "think", "content": content}
                        messages.append({"role": "assistant", "content": content})
                        turn_type = "call"
                        continue
                turn_type = "call"
            except Exception as e:
                yield {"type": "think", "content": f"[error - think] {e}"}
                break

        else:  # call turn
            try:
                response = await client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    temperature=temperature,
                    top_p=top_p,
                    presence_penalty=presence_penalty,
                    tools=tools,
                    tool_choice={"type": "function", "function": {"name": "request_for_expert"}},
                )

                assistant_message = response.choices[0].message
                content = assistant_message.content or ""
                tool_calls = assistant_message.tool_calls or []

                # ê°„í—ì  í…ìŠ¤íŠ¸ëŠ” reasoningì— í•©ë¥˜í•˜ê±°ë‚˜ ìµœì¢… ì‘ë‹µ íŒë‹¨ (ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬ í›„ ì¢…ë£Œ)
                has_final_response = False
                final_response_text = ""
                
                if content:
                    if "</think>" in content:
                        before, after = content.split("</think>", 1)
                        reasoning_text = before
                        if "<think>" in reasoning_text:
                            reasoning_text = reasoning_text.split("<think>", 1)[1]
                        if reasoning_text.strip():
                            yield {"type": "think", "content": reasoning_text.strip()}
                        
                        after_text = after.strip()
                        # ë„êµ¬ í˜¸ì¶œì´ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸ (ë°”ë¡œ breakí•˜ì§€ ì•ŠìŒ)
                        if after_text and not after_text.startswith("<tool"):
                            has_final_response = True
                            final_response_text = after_text
                    else:
                        yield {"type": "think", "content": content}

                msg_to_add = {"role": "assistant", "content": content}
                if tool_calls:
                    msg_to_add["tool_calls"] = [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {"name": tc.function.name, "arguments": tc.function.arguments},
                        }
                        for tc in tool_calls
                    ]
                messages.append(msg_to_add)

                if tool_calls:
                    for tool_call in tool_calls:
                        try:
                            args = json.loads(tool_call.function.arguments)
                        except Exception:
                            args = {"input": tool_call.function.arguments}
                        # ë„êµ¬ í˜¸ì¶œ ì§ˆë¬¸ë„ í•¨ê»˜ ì „ë‹¬
                        tool_input = args.get("input", "")
                        tool_output_content = await execute_request_for_expert(tool_input)
                        messages.append({
                            "tool_call_id": tool_call.id,
                            "role": "tool",
                            "name": tool_call.function.name,
                            "content": tool_output_content,
                        })

                        yield {"type": "tool_output", "content": tool_output_content, "input": tool_input}

                    # ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬ í›„ ìµœì¢… ì‘ë‹µì´ ìˆë‹¤ë©´ ì¶œë ¥í•˜ê³  ì¢…ë£Œ
                    if has_final_response:
                        yield {"type": "final", "content": final_response_text}
                        break
                    else:
                        turn_type = "think"  # ë‹¤ìŒ ì‚¬ê³  í„´ìœ¼ë¡œ
                else:
                    # ë„êµ¬ í˜¸ì¶œì´ ì—†ëŠ” ê²½ìš°
                    if has_final_response:
                        yield {"type": "final", "content": final_response_text}
                        break
                    elif content:
                        # ìµœì¢… ì‘ë‹µë„ ì•„ë‹ˆê³  ë„êµ¬ í˜¸ì¶œë„ ì—†ìœ¼ë©´ ì¢…ë£Œ
                        yield {"type": "final", "content": content}
                        break
                    else:
                        break
            except Exception as e:
                yield {"type": "think", "content": f"[error - call] {e}"}
                break

    # ìµœì¢… í…ìŠ¤íŠ¸ ê²°ì •
    if not final_text:
        for msg in reversed(messages):
            if msg.get("role") == "assistant" and msg.get("content"):
                final_text = msg["content"]
                break
    yield {"type": "final", "content": final_text or ""}


# --- ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ë¡œì§ ---
async def stream_and_render():
    try:
        # í† í¬ë‚˜ì´ì €ëŠ” ì„ íƒ ì‚¬í•­ (ê¸¸ì´ ê³„ì‚° ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ë¬´ì‹œ)
        try:
            _ = AutoTokenizer.from_pretrained(tokenizer_path)
        except Exception:
            pass

        # ê° ì´ë²¤íŠ¸ë§ˆë‹¤ ìƒˆë¡œìš´ ì»¨í…Œì´ë„ˆë¥¼ ìƒì„±í•´ì„œ ë³µì œ ë¬¸ì œ ë°©ì§€
        event_containers = []
        
        async for event in run_tool_call_flow_streaming(
            model_name=model_name,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=temperature,
            top_p=top_p,
            presence_penalty=presence_penalty,
        ):
            etype = event.get("type")
            
            # ê¸°ì¡´ placeholderë¥¼ ë¹„ìš°ê³  ìƒˆë¡œìš´ ì»¨í…Œì´ë„ˆ ìƒì„±
            with placeholder.container():
                # ì´ì „ ì´ë²¤íŠ¸ë“¤ì„ ë‹¤ì‹œ ë Œë”ë§
                for prev_event in event_containers:
                    render_event(prev_event)
                
                # í˜„ì¬ ì´ë²¤íŠ¸ ë Œë”ë§
                render_event(event)
                
                # ì´ë²¤íŠ¸ ì €ì¥
                event_containers.append(event)
                
                if etype == "final":
                    break

    except Exception as e:
        st.error(f"ëª¨ë¸ í˜¸ì¶œ ë˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def render_event(event):
    """ê°œë³„ ì´ë²¤íŠ¸ë¥¼ ë Œë”ë§í•˜ëŠ” í•¨ìˆ˜"""
    etype = event.get("type")
    
    if etype == "think":
        st.markdown("### ğŸ¤” Reasoning:")
        st.markdown(f'<div class="passage-font">{format_text_to_html((event.get("content") or "").strip())}</div>', unsafe_allow_html=True)
        
    elif etype == "tool_output":
        out_text = (event.get("content") or "").strip()
        input_text = (event.get("input") or "").strip()
        st.markdown("### ğŸ” Request for Expert")
        with st.expander("ğŸ“¤ Question to Expert", expanded=False):
            st.markdown(input_text)
        with st.expander("ğŸ§  Expert's response", expanded=False):
            st.markdown(out_text)

    elif etype == "final":
        st.markdown("### âœ… Final Response:")
        st.markdown(f'<div class="passage-font">{format_text_to_html((event.get("content") or "").strip())}</div>', unsafe_allow_html=True)


if run_button:
    # ë²„íŠ¼ì´ í´ë¦­ë˜ë©´ ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜ë¥¼ ì‹¤í–‰
    asyncio.run(stream_and_render())
